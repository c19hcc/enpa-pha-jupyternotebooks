{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2231bc45",
   "metadata": {},
   "source": [
    "# Roll Your Own Analysis: Debiasing Raw Date Exposure Data\n",
    "\n",
    "This notebook will demonstrate how to download raw data from the ENPA REST API, debias date exposure data, and display a basic visualization by performing the following actions:\n",
    "\n",
    "1. <a href=\"#Downloading-Raw-ENPA-Data\">Downloading raw ENPA data</a>, \n",
    "2. <a href=\"#Debiasing-Raw-Data\">Debiasing raw date exposure data</a>, and\n",
    "3. <a href=\"#Visualizing-the-Debiased-Data\">Visualizing the debiased data</a>.\n",
    "\n",
    "Be sure to have your full API key ready before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf51f8",
   "metadata": {},
   "source": [
    "## Downloading Raw ENPA Data\n",
    "\n",
    "This section will describe how to download raw ENPA data to perform further analysis using your full API key. Begin by following the steps below.\n",
    "\n",
    "1) Navigate to the *API Docs* tab within the ENPA portal, and select *Authorize*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118807f",
   "metadata": {},
   "source": [
    "![Step4](https://raw.githubusercontent.com/c19hcc/enpa-pha-jupyternotebooks/main/images/Step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146f160",
   "metadata": {},
   "source": [
    "2) Input your *Full Key* into the dialog box and select *Authorize*. Note, the full API key must be included as an `X-Api-Key` header in all requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db4802",
   "metadata": {},
   "source": [
    "![Step5W](https://raw.githubusercontent.com/c19hcc/enpa-pha-jupyternotebooks/main/images/Step5W.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc8e10",
   "metadata": {},
   "source": [
    "3) Import Python's Request library and construct the necessary header to include the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "header = {\"X-Api-Key\": # \" insert API key here between the quotation marks \"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23c254",
   "metadata": {},
   "source": [
    "4) Specify parameters and request the desired data.\n",
    "* `datasets`: Choose from one of several datasets to download :`notification`, `notificationInteractions`, `riskParameters`, `codeVerified`, `keysUploaded`, `beaconCount`, `dateExposure`, `dateExposure14d`, `dateExposureV2`, `keysUploadedVaccineStatus` , `keysUploadedVaccineStatus14d`, `keysUploadedVaccineStatusV2`, `keysUploadedWithReportType`, `keysUploadedWithReportType14d`, `periodicExposureNotification`, `periodicExposureNotification14d`, `secondaryAttack`, `secondaryAttack14d`, or `userRisk`.\n",
    "* `raw`: Specify `True` to return the raw, biased data or `False` to return the debiased data.\n",
    "* `start_date`: Specify a start date to download data (must be in YYYY-MM-DD format); if omitted, the default is 90 days before either the end date (if specified) or the current date.\n",
    "* `end_date`: Specify an end date to download data (must be in YYYY-MM-DD format); if omitted, the default is the current date. \n",
    "* `country`: Input an ISO 3166-1 code for the country (e.g., `US`).\n",
    "* `state`: Input an ISO-3166-2 code for the subdivision, state, or province (e.g., `US-VA`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"datasets\": \"dateExposure14d\",\n",
    "    \"raw\": True,\n",
    "    \"start_date\": \"2022-03-31\",\n",
    "    \"end_date\": \"2022-04-06\",\n",
    "    \"country\": \"US\",\n",
    "    \"state\": \"US-VA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d81c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DERequest = requests.get(\"https://api.enpa-pha.io/api/public/v2/enpa-data\",\n",
    "                      headers = header, params = parameters)\n",
    "print(\"Reason: \", DERequest.reason, \"\\nStatus Code: \", DERequest.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16773706",
   "metadata": {},
   "source": [
    "A *Status Code* of 200 indicates that the data was successfully retrieved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f91ec",
   "metadata": {},
   "source": [
    "## Debiasing Raw Data\n",
    "\n",
    "This section will describe how to prepare and debias the raw data downloaded from ENPA's REST API. Begin by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fd422",
   "metadata": {},
   "source": [
    "1) Convert the downloaded data from string to dictionary form using the `json` library, so it can be easy manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32031eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "convertedDE = json.loads(DERequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135d8c4",
   "metadata": {},
   "source": [
    "Here is what the data look like. Note that `DateExposure` uses a cumulative distribution for the values. For each exposure notification classification, there are 12 buckets. For example, for classification 1 notifications, buckets 0-11 are used:\n",
    "* Bucket 0 - delay is 0 days\n",
    "* Bucket 1 - delay is 0-1 days\n",
    "* Bucket 2 - delay is 0-2 days\n",
    "* ...\n",
    "* Bucket 10 - delay is 0-10 days\n",
    "* Bucket 11 - any delay\n",
    "\n",
    "Similarly, for a classification 2 exposure, buckets 12-23 are used. Classification 3 uses buckets 24-35, and classification 4 uses buckets 36-47."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00733c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convertedDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f84764",
   "metadata": {},
   "source": [
    "2) Crawling through the dictionary structure, we need to find the `totalCount`, the total number of clients (`total_individual_clients`); `sum`, the sum value (sum of an individual element of the `sum` array(s)); and `epsilon`, the epsilon from the raw value. These values are needed to debias the data. Here is what the debias function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getMostLikelyPopulationCount(totalCount, sumPart, epsilon):\n",
    "    '''\n",
    "    Debiases raw sum values (`sumPart`).\n",
    "    \n",
    "    :param totalCount: The total number of clients (`total_individual_clients`)\n",
    "    :param sumPart: The sum value (sum of an individual element of the `sum` array(s))\n",
    "    :param epsilon: Epsilon from the raw value\n",
    "    :return: \n",
    "        - `mostLikelyPopulation` (float): The debiased (expected) value\n",
    "        - `standardDeviation` (float): Standard deviation\n",
    "    '''\n",
    "    p = 1 / (1 + math.exp(epsilon))\n",
    "    sqrtPTimesOneMinusP = math.exp(epsilon / 2) / (1 + math.exp(epsilon))\n",
    "    mostLikelyPopulation = (sumPart - totalCount * p) / (1 - 2 * p)\n",
    "    standardDeviation = math.sqrt(totalCount) * sqrtPTimesOneMinusP\n",
    "    return mostLikelyPopulation, standardDeviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9f107",
   "metadata": {},
   "source": [
    "3) Let's debias the notification data by leveraging the debias function. We will create a dataframe aggregated by date with the debiased Apple and Google Notification data. Let's work with the first 7 bins (i.e., up to a 0-7 day delay) for type 1 notifications. Since this metric represents a percentage, we will need to include the last bin (bin 11) to serve as the divisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb869f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Lists for Data Storage\n",
    "datesApple, totalCountApple, epsilonApple, dateExposureApple = [], [], [], []\n",
    "datesGoogle, totalCountGoogle, epsilonGoogle, dateExposureGoogle = [], [], [], []\n",
    "SPG1, SPG2, SPG3, SPG4, SPG5, SPG6, SPG7, SPG8, SPGDivisor = [], [], [], [], [], [], [], [], []\n",
    "SPA1, SPA2, SPA3, SPA4, SPA5, SPA6, SPA7, SPA8, SPADivisor = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(len(convertedDE['rawData'])):\n",
    "    # Data Collection to Debiasing Apple/iOS Data\n",
    "    if(convertedDE['rawData'][i]['aggregation_id'] == \"com.apple.EN.DateExposureV2D14\"):\n",
    "        datesApple.append(datetime.strptime(convertedDE['rawData'][i]['aggregation_start_time'][0:10], \"%Y-%m-%d\"))\n",
    "        totalCountApple.append(convertedDE['rawData'][i]['total_individual_clients']) # totalCount parameter for debiasing\n",
    "        SPA1.append(convertedDE['rawData'][i]['sum'][0]) # sumPart 0 day delay\n",
    "        SPA2.append(convertedDE['rawData'][i]['sum'][1]) # sumPart 0-1 day delay\n",
    "        SPA3.append(convertedDE['rawData'][i]['sum'][2]) # sumPart 0-2 day delay\n",
    "        SPA4.append(convertedDE['rawData'][i]['sum'][3]) # sumPart 0-3 day delay\n",
    "        SPA5.append(convertedDE['rawData'][i]['sum'][4]) # sumPart 0-4 day delay\n",
    "        SPA6.append(convertedDE['rawData'][i]['sum'][5]) # sumPart 0-5 day delay\n",
    "        SPA7.append(convertedDE['rawData'][i]['sum'][6]) # sumPart 0-6 day delay\n",
    "        SPA8.append(convertedDE['rawData'][i]['sum'][7]) # sumPart 0-7 day delay\n",
    "        SPADivisor.append(convertedDE['rawData'][i]['sum'][11]) # total cumulative sumPart delay\n",
    "        epsilonApple.append(convertedDE['rawData'][i]['epsilon']) # epsilon parameter for debiasing\n",
    "        \n",
    "    # Data Collection to Debiasing Google/Android Data\n",
    "    elif(convertedDE['rawData'][i]['aggregation_id'] == \"DateExposure14d\"):\n",
    "        datesGoogle.append(datetime.strptime(convertedDE['rawData'][i]['aggregation_start_time'][0:10], \"%Y-%m-%d\"))\n",
    "        totalCountGoogle.append(convertedDE['rawData'][i]['total_individual_clients']) # totalCount parameter for debiasing\n",
    "        SPG1.append(convertedDE['rawData'][i]['sum'][0]) # sumPart 0 day delay\n",
    "        SPG2.append(convertedDE['rawData'][i]['sum'][1]) # sumPart 0-1 day delay\n",
    "        SPG3.append(convertedDE['rawData'][i]['sum'][2]) # sumPart 0-2 day delay\n",
    "        SPG4.append(convertedDE['rawData'][i]['sum'][3]) # sumPart 0-3 day delay\n",
    "        SPG5.append(convertedDE['rawData'][i]['sum'][4]) # sumPart 0-4 day delay\n",
    "        SPG6.append(convertedDE['rawData'][i]['sum'][5]) # sumPart 0-5 day delay\n",
    "        SPG7.append(convertedDE['rawData'][i]['sum'][6]) # sumPart 0-6 day delay\n",
    "        SPG8.append(convertedDE['rawData'][i]['sum'][7]) # sumPart 0-7 day delay\n",
    "        SPGDivisor.append(convertedDE['rawData'][i]['sum'][11]) # total cumulative sumPart delay\n",
    "        epsilonGoogle.append(convertedDE['rawData'][i]['epsilon']) # epsilon parameter for debiasing\n",
    "\n",
    "# Place Apple data in a dataframe.\n",
    "RawAppleData = pd.DataFrame(list(zip(datesApple, totalCountApple, SPA1, SPA2, SPA3, SPA4, SPA5, SPA6, \n",
    "                                     SPA7, SPA8, SPADivisor, epsilonApple)), \n",
    "                            columns = ['Date', 'Total', 'SumPartApple0', 'SumPartApple1', 'SumPartApple2',\n",
    "                                       'SumPartApple3','SumPartApple4', 'SumPartApple5','SumPartApple6', \n",
    "                                       'SumPartApple7', 'SumPartDivisor', 'Epsilon'])        \n",
    "\n",
    "# Aggregate the sumPart bins for each day for the Apple. \n",
    "# Epsilon is segregated since it should not be summed.\n",
    "epsPart = ['Epsilon']\n",
    "sumParts = RawAppleData.columns.difference(epsPart).tolist()\n",
    "\n",
    "# Perform Groupby Operations on Apple Data\n",
    "g = RawAppleData.groupby(['Date'])\n",
    "sumPartsGrouped = g[sumParts].sum()\n",
    "epsPartGrouped = g[epsPart].mean() # Epsilon will be constant throughout the day.\n",
    "RawAppleGrouping = pd.concat([sumPartsGrouped, epsPartGrouped], axis = 1)\n",
    "  \n",
    "# Place Google data in a dataframe.\n",
    "RawGoogleData = pd.DataFrame(list(zip(datesGoogle, totalCountGoogle, SPG1, SPG2, SPG3, SPG4, SPG5, SPG6, \n",
    "                                      SPG7, SPG8, SPGDivisor, epsilonGoogle)), \n",
    "                             columns = ['Date', 'Total', 'SumPartGoogle0', 'SumPartGoogle1', 'SumPartGoogle2',\n",
    "                                        'SumPartGoogle3','SumPartGoogle4', 'SumPartGoogle5','SumPartGoogle6', \n",
    "                                        'SumPartGoogle7', 'SumPartDivisor', 'Epsilon'])     \n",
    "\n",
    "# Aggregate the sumPart bins for each day for the Google data. \n",
    "# Epsilon is segregated since it should not be summed.\n",
    "epsPart = ['Epsilon']\n",
    "sumParts = RawGoogleData.columns.difference(epsPart).tolist()\n",
    "\n",
    "# Perform Groupby Operations on Apple Data\n",
    "g = RawGoogleData.groupby(['Date'])\n",
    "sumPartsGrouped = g[sumParts].sum()\n",
    "epsPartGrouped = g[epsPart].mean() # Epsilon will be constant throughout the day.\n",
    "RawGoogleGrouping = pd.concat([sumPartsGrouped, epsPartGrouped], axis = 1)\n",
    "\n",
    "# Debias the Apple Notification Data\n",
    "for i in range(RawAppleGrouping.shape[0]):\n",
    "    temp = []\n",
    "    for c in range(0, 9):\n",
    "        sumPart = RawAppleGrouping.iloc[i,c]\n",
    "        (mostLikelyDE, standardDeviation) = getMostLikelyPopulationCount(RawAppleGrouping.Total[i], \n",
    "                                                                         sumPart, RawAppleGrouping.Epsilon[i])\n",
    "        temp.append(mostLikelyDE)\n",
    "    dateExposureApple.append(temp)\n",
    "\n",
    "# Debias the Google Notification Data\n",
    "for i in range(RawGoogleGrouping.shape[0]):\n",
    "    temp = []\n",
    "    for c in range(0, 9):\n",
    "        sumPart = RawGoogleGrouping.iloc[i,c]\n",
    "        (mostLikelyDE, standardDeviation) = getMostLikelyPopulationCount(RawGoogleGrouping.Total[i], \n",
    "                                                                         sumPart, RawGoogleGrouping.Epsilon[i])\n",
    "        temp.append(mostLikelyDE)\n",
    "    dateExposureGoogle.append(temp)\n",
    "\n",
    "# Clean Up Data\n",
    "RawAppleGrouping['DebiasedAppleDE'] = dateExposureApple\n",
    "RawGoogleGrouping['DebiasedGoogleDE'] = dateExposureGoogle\n",
    "DebiasedData = pd.DataFrame(list(zip(RawAppleGrouping.index, RawAppleGrouping.DebiasedAppleDE, \n",
    "                                     RawGoogleGrouping.DebiasedGoogleDE)),\n",
    "                           columns = ['Date', 'Debiased Apple Date Exposure', 'Debiased Google Date Exposure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cde23",
   "metadata": {},
   "source": [
    "Here is what the debiased data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "DebiasedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ff081",
   "metadata": {},
   "source": [
    "4) All that is left is combining the datasets appropriately. Begin by combining the Apple and Google data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de354ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalDE = []\n",
    "for r in range(DebiasedData.shape[0]):\n",
    "    totalStaging = []\n",
    "    for i in range(0, len(DebiasedData.iloc[r, 1])):\n",
    "        totalStaging.append(DebiasedData.iloc[r, 1][i] + DebiasedData.iloc[r, 2][i])\n",
    "    TotalDE.append(totalStaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b649f",
   "metadata": {},
   "source": [
    "Here are what the finalized data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfff6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MergedData = DebiasedData.copy()\n",
    "MergedData['Combined Date Exposure'] = TotalDE\n",
    "MergedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532f80d",
   "metadata": {},
   "source": [
    "Finally divide each bin by the last bin, the divisor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92abe7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the bins in the dataframe\n",
    "DateExposure = pd.DataFrame(MergedData['Combined Date Exposure'].to_list(), columns=['Raw Bin 0', 'Raw Bin 1',\n",
    "                                                                                     'Raw Bin 2', 'Raw Bin 3', \n",
    "                                                                                     'Raw Bin 4', 'Raw Bin 5',\n",
    "                                                                                     'Raw Bin 6', 'Raw Bin 7',\n",
    "                                                                                     'Raw Bin 11'])\n",
    "# Calculate the cumulative percentages\n",
    "DateExposure['Bin 0'] = DateExposure['Raw Bin 0'] / DateExposure['Raw Bin 11'] * 100\n",
    "DateExposure['Bin 1'] = DateExposure['Raw Bin 1'] / DateExposure['Raw Bin 11'] * 100\n",
    "DateExposure['Bin 2'] = DateExposure['Raw Bin 2'] / DateExposure['Raw Bin 11'] * 100\n",
    "DateExposure['Bin 3'] = DateExposure['Raw Bin 3'] / DateExposure['Raw Bin 11'] * 100\n",
    "DateExposure['Bin 4'] = DateExposure['Raw Bin 4'] / DateExposure['Raw Bin 11'] * 100\n",
    "DateExposure['Bin 5'] = DateExposure['Raw Bin 5'] / DateExposure['Raw Bin 11'] * 100\n",
    "DateExposure['Bin 6'] = DateExposure['Raw Bin 6'] / DateExposure['Raw Bin 11'] * 100\n",
    "DateExposure['Bin 7'] = DateExposure['Raw Bin 7'] / DateExposure['Raw Bin 11'] * 100\n",
    "DateExposure = DateExposure.mask(DateExposure < 0, 0)\n",
    "DateExposure['Date'] = MergedData['Date'].dt.strftime('%m/%d/%Y')\n",
    "DateExposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175a478",
   "metadata": {},
   "source": [
    "## Visualizing the Debiased Data\n",
    "\n",
    "This section will describe how to prepare and visualize the raw data downloaded from ENPA's REST API. Begin by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404c075",
   "metadata": {},
   "source": [
    "1) Run the code below to construct the bar chart. The dataframe, `DateExposure`, is already organized, so it is easy to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2198b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create the base axis to add the bars\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "\n",
    "# Font size of the x tick label\n",
    "plt.rc('xtick', labelsize=11) \n",
    "\n",
    "# Extract the date for the x labels\n",
    "label = DateExposure['Date']\n",
    "x = np.arange(len(label))\n",
    "\n",
    "# Specifies the bar width\n",
    "width = 0.22\n",
    "\n",
    "# Create bars and offset them, so they are not displayed on top of one another\n",
    "rect0 = ax.bar(x - width, DateExposure['Bin 2'], width = width, label = \"0-2 Day Delay\", edgecolor = \"black\")\n",
    "rect1 = ax.bar(x, DateExposure['Bin 4'], width = width, label = \"0-4 Day Delay\", edgecolor = \"black\")\n",
    "rect2 = ax.bar(x + width, DateExposure['Bin 6'], width = width, label = \"0-6 Day Delay\", edgecolor = \"black\")\n",
    "\n",
    "# Add labels\n",
    "ax.set_ylabel(\"Percent of Type 1 Notifications\", fontsize = 20)\n",
    "ax.set_xlabel(\"Date\", fontsize = 20)\n",
    "ax.set_title(\"Date Exposure: Percent of Type 1 Notifications\", fontsize = 25)\n",
    "\n",
    "# Use the date as the x labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(label)\n",
    "ax.tick_params(axis = \"x\", which = \"both\", labelrotation = 65)\n",
    "\n",
    "# Set the legend font size\n",
    "ax.legend(fontsize = 14)\n",
    "\n",
    "# Add gridlines and an axis line\n",
    "ax.grid(which='major', linestyle=':', linewidth='0.2', color='black')\n",
    "plt.axhline(0, color='black', ls='solid');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c747e3",
   "metadata": {},
   "source": [
    "2) Next, save the plot to your current directory. The filename will include the date range of the plot (e.g., `DateExposurePlot_2022-01-01_to_2022-01-31.pdf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = datetime.strftime(MergedData['Date'][0], \"%Y-%m-%d\")\n",
    "endDate = datetime.strftime(MergedData['Date'][len(MergedData['Date']) - 1], \"%Y-%m-%d\")\n",
    "fig.tight_layout()\n",
    "ax.figure.savefig(f'DateExposurePlot_{startDate}_to_{endDate}.pdf', format='pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6434dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
