{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2231bc45",
   "metadata": {},
   "source": [
    "# Roll Your Own Analysis\n",
    "\n",
    "This notebook will demonstrate how to download raw data from the ENPA REST API, debias it, and display a basic visualization by performing the following actions:\n",
    "\n",
    "1. <a href=\"#Downloading-Raw-ENPA-Data\">Downloading raw ENPA data</a>, \n",
    "2. <a href=\"#Debiasing-Raw-Data\">Debiasing raw data</a>, and \n",
    "3. <a href=\"#Visualizing-the-Debiased-Data\">Visualizing the debiased data</a>.\n",
    "\n",
    "Be sure to have your full API key ready before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf51f8",
   "metadata": {},
   "source": [
    "## Downloading Raw ENPA Data\n",
    "\n",
    "This section will describe how to download raw ENPA data to perform further analysis using your full API key. Begin by following the steps below.\n",
    "\n",
    "1) Navigate to the *API Docs* tab within the ENPA portal, and select *Authorize*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118807f",
   "metadata": {},
   "source": [
    "![Step4](https://raw.githubusercontent.com/c19hcc/enpa-pha-jupyternotebooks/main/images/Step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146f160",
   "metadata": {},
   "source": [
    "2) Input your *Full Key* into the dialog box and select *Authorize*. Note, the full API key must be included as an `X-Api-Key` header in all requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db4802",
   "metadata": {},
   "source": [
    "![Step5W](https://raw.githubusercontent.com/c19hcc/enpa-pha-jupyternotebooks/main/images/Step5W.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc8e10",
   "metadata": {},
   "source": [
    "3) Import Python's Request library and construct the necessary header to include the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "myFullKey = # \" Insert your full key here \"\n",
    "\n",
    "header = {\"X-Api-Key\": myFullKey}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23c254",
   "metadata": {},
   "source": [
    "4) Specify parameters and request the desired data.\n",
    "* `datasets`: Choose from one of several datasets to download :`notification`, `notificationInteractions`, `riskParameters`, `codeVerified`, `keysUploaded`, `beaconCount`, `dateExposure`, `dateExposure14d`, `dateExposureV2`, `keysUploadedVaccineStatus` , `keysUploadedVaccineStatus14d`, `keysUploadedVaccineStatusV2`, `keysUploadedWithReportType`, `keysUploadedWithReportType14d`, `periodicExposureNotification`, `periodicExposureNotification14d`, `secondaryAttack`, `secondaryAttack14d`, or `userRisk`.\n",
    "* `raw`: Specify `True` to return the raw, biased data or `False` to return the debiased data.\n",
    "* `start_date`: Specify a start date to download data (must be in YYYY-MM-DD format); if omitted, the default is 90 days before either the end date (if specified) or the current date.\n",
    "* `end_date`: Specify an end date to download data (must be in YYYY-MM-DD format); if omitted, the default is the current date. \n",
    "* `country`: Input an ISO 3166-1 code for the country (e.g., `US`).\n",
    "* `state`: Input an ISO-3166-2 code for the subdivision, state, or province (e.g., `US-VA`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"datasets\": \"notification\",\n",
    "    \"raw\": True,\n",
    "    \"start_date\": \"2022-01-01\",\n",
    "    \"end_date\": \"2022-01-31\",\n",
    "    \"country\": \"US\",\n",
    "    \"state\": \"US-VA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d81c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "notificationRequest = requests.get(\"https://api.enpa-pha.io/api/public/v2/enpa-data\",\n",
    "                      headers = header, params = parameters)\n",
    "print(\"Reason: \", notificationRequest.reason, \"\\nStatus Code: \", notificationRequest.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16773706",
   "metadata": {},
   "source": [
    "A *Status Code* of 200 indicates that the data was successfully retrieved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7126c6",
   "metadata": {},
   "source": [
    "5) In addition to requesting the `notification` dataset, we also need to request the `userRisk` dataset to help correct some of the Apple iOS values in the raw data. Use the same parameters as you did in step 4, but change the `datasets` parameter to `userRisk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fe036",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"datasets\": \"userRisk\",\n",
    "    \"raw\": True,\n",
    "    \"start_date\": \"2022-01-01\",\n",
    "    \"end_date\": \"2022-01-31\",\n",
    "    \"country\": \"US\",\n",
    "    \"state\": \"US-VA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54763b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "userRiskRequest = requests.get(\"https://api.enpa-pha.io/api/public/v2/enpa-data\",\n",
    "                      headers = header, params = parameters)\n",
    "print(\"Reason: \", userRiskRequest.reason, \"\\nStatus Code: \", userRiskRequest.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ba962",
   "metadata": {},
   "source": [
    "A *Status Code* of 200 indicates that the data was successfully retrieved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f91ec",
   "metadata": {},
   "source": [
    "## Debiasing Raw Data\n",
    "\n",
    "This section will describe how to prepare and debias the raw data downloaded from ENPA's REST API. Begin by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fd422",
   "metadata": {},
   "source": [
    "1) Convert the downloaded data from string to dictionary form using the `json` library, so it can be easy manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32031eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "convertedNotifications = json.loads(notificationRequest.text)\n",
    "convertedUserRisk = json.loads(userRiskRequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135d8c4",
   "metadata": {},
   "source": [
    "Here is what the data look like. Let's try to visualize total number of notifications over time as a stacked bar chart indicating iOS and Android notifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00733c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convertedNotifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertedUserRisk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f84764",
   "metadata": {},
   "source": [
    "2) Crawling through the dictionary structure, we need to find the `totalCount`, the total number of clients (`total_individual_clients`); `sum`, the sum value (sum of an individual element of the `sum` array(s)); and `epsilon`, the epsilon from the raw value. These values are needed to debias the data. Here is what the debias function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getMostLikelyPopulationCount(totalCount, sumPart, epsilon):\n",
    "    '''\n",
    "    Debiases raw sum values (`sumPart`).\n",
    "    \n",
    "    :param totalCount: The total number of clients (`total_individual_clients`)\n",
    "    :param sumPart: The sum value (sum of an individual element of the `sum` array(s))\n",
    "    :param epsilon: Epsilon from the raw value\n",
    "    :return: \n",
    "        - `mostLikelyPopulation` (float): The debiased (expected) value\n",
    "        - `standardDeviation` (float): Standard deviation\n",
    "    '''\n",
    "    p = 1 / (1 + math.exp(epsilon))\n",
    "    sqrtPTimesOneMinusP = math.exp(epsilon / 2) / (1 + math.exp(epsilon))\n",
    "    mostLikelyPopulation = (sumPart - totalCount * p) / (1 - 2 * p)\n",
    "    standardDeviation = math.sqrt(totalCount) * sqrtPTimesOneMinusP\n",
    "    return mostLikelyPopulation, standardDeviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931533e",
   "metadata": {},
   "source": [
    "3) Let's debias the notification data by leveraging the debias function. We will create a dataframe aggregated by date with the debiased Apple and Google Notification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35409a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Arrays for Data Storage\n",
    "datesApple, totalCountApple, sumPartApple, epsilonApple, notificationsApple = [], [], [], [], []\n",
    "datesGoogle, totalCountGoogle, sumPartGoogle, epsilonGoogle, notificationsGoogle = [], [], [], [], []\n",
    "\n",
    "for i in range(len(convertedNotifications['rawData'])):\n",
    "    # Data Collection to Debiasing Apple/iOS Data\n",
    "    if(convertedNotifications['rawData'][i]['aggregation_id'] == \"com.apple.EN.UserNotification\"):\n",
    "        datesApple.append(datetime.strptime(convertedNotifications['rawData'][i]['aggregation_start_time'][0:10], \"%Y-%m-%d\"))\n",
    "        totalCountApple.append(convertedNotifications['rawData'][i]['total_individual_clients']) # totalCount parameter for debiasing\n",
    "        sumPartApple.append(convertedNotifications['rawData'][i]['sum'][1]) # sumPart parameter for debiasing\n",
    "        epsilonApple.append(convertedNotifications['rawData'][i]['epsilon']) # epsilon parameter for debiasing\n",
    "        \n",
    "    # Data Collection to Debiasing Google/Android Data\n",
    "    elif(convertedNotifications['rawData'][i]['aggregation_id'] == \"PeriodicExposureNotification\"):\n",
    "        datesGoogle.append(datetime.strptime(convertedNotifications['rawData'][i]['aggregation_start_time'][0:10], \"%Y-%m-%d\"))\n",
    "        totalCountGoogle.append(convertedNotifications['rawData'][i]['total_individual_clients']) # totalCount parameter for debiasing\n",
    "        sumPartGoogle.append(convertedNotifications['rawData'][i]['sum'][1]) # sumPart parameter for debiasing\n",
    "        epsilonGoogle.append(convertedNotifications['rawData'][i]['epsilon']) # epsilon parameter for debiasing\n",
    "        \n",
    "# Aggregate Notification data by Date\n",
    "RawAppleData = pd.DataFrame(list(zip(datesApple, totalCountApple, sumPartApple, epsilonApple)), columns = ['Date', 'Total', 'SumPart', 'Epsilon'])        \n",
    "RawAppleGrouping = RawAppleData.groupby(['Date'])[['Total','SumPart']].sum()    \n",
    "RawGoogleData = pd.DataFrame(list(zip(datesGoogle, totalCountGoogle, sumPartGoogle, epsilonGoogle)), columns = ['Date', 'Total', 'SumPart', 'Epsilon'])        \n",
    "RawGoogleGrouping = RawGoogleData.groupby(['Date'])[['Total','SumPart']].sum()    \n",
    "\n",
    "# Debias the Apple Notification Data\n",
    "for i in range(RawAppleGrouping.shape[0]):\n",
    "    (mostLikelyNotifications, standardDeviation) = getMostLikelyPopulationCount(RawAppleGrouping.Total[i], RawAppleGrouping.SumPart[i], 8)\n",
    "    notificationsApple.append(mostLikelyNotifications)\n",
    "\n",
    "# Debias the Google Notification Data\n",
    "for i in range(RawGoogleGrouping.shape[0]):\n",
    "    (mostLikelyNotifications, standardDeviation) = getMostLikelyPopulationCount(RawGoogleGrouping.Total[i], RawGoogleGrouping.SumPart[i], 8)\n",
    "    notificationsGoogle.append(mostLikelyNotifications)\n",
    "\n",
    "# Clean Up Data\n",
    "RawAppleGrouping['DebiasedAppleNotifications'] = notificationsApple\n",
    "RawGoogleGrouping['DebiasedGoogleNotifications'] = notificationsGoogle\n",
    "DebiasedData = pd.DataFrame(list(zip(RawAppleGrouping.index, RawAppleGrouping.DebiasedAppleNotifications, RawGoogleGrouping.DebiasedGoogleNotifications)), columns = ['Date', 'Apple Notifications', 'Google Notifications'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c09d73",
   "metadata": {},
   "source": [
    "Here is what the Debiased data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f835875",
   "metadata": {},
   "outputs": [],
   "source": [
    "DebiasedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad67545",
   "metadata": {},
   "source": [
    "4) Before visualizating the data, there is one more step to prepare the data. The Apple Notification data needs to be altered slightly more to fully debias the data. It is now time to debias and scale the `userRisk` dataset that was requested in Step 5 of the <a href=\"#Downloading-Raw-ENPA-Data\">Downloading Raw ENPA Data</a> section. We will start by calling the `debiasAndScale` function which will perform the heavy lifting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debiasAndScale(userRiskData, startPlace, endPlace):\n",
    "    '''\n",
    "    Debiases and determines the adjustment ratio from the userRisk dataset.\n",
    "    This function is run once for each day in the dataset.\n",
    "    \n",
    "    :param userRiskData: The userRisk dataset of interest.\n",
    "    :param startPlace: The first occurance of a particular day.\n",
    "    :param endPlace: The last occurance of a particular day.\n",
    "    :return: \n",
    "        - `ratio` (float): The adjustment ratio for Apple Notification counts\n",
    "        - `date` (float): The adjustment ratio's corresponding date for bookkeeping purposes\n",
    "    '''\n",
    "    newSumPart = np.zeros(len(userRiskData['rawData'][0]['sum']))\n",
    "    totalClients = []\n",
    "    debiasedUserRisk = []\n",
    "\n",
    "    for i in range(startPlace, endPlace):\n",
    "        totalClients.append(userRiskData['rawData'][i]['total_individual_clients'])\n",
    "        for j in range(len(userRiskData['rawData'][0]['sum'])):\n",
    "            newSumPart[j] += userRiskData['rawData'][i]['sum'][j]\n",
    "        epsilon = userRiskData['rawData'][i]['epsilon']\n",
    "\n",
    "    # Debias UserRisk\n",
    "    for i in range(len(userRiskData['rawData'][0]['sum'])):\n",
    "        sumPart = newSumPart[i]\n",
    "        (mostLikelyRisk, stdev) = getMostLikelyPopulationCount(sum(totalClients), sumPart, epsilon)\n",
    "        debiasedUserRisk.append(mostLikelyRisk)\n",
    "\n",
    "    # Scale UserRisk\n",
    "    ratio = sum(totalClients)/sum(debiasedUserRisk)\n",
    "    date = datetime.strptime(userRiskData['rawData'][startPlace]['aggregation_start_time'][0:10], \"%Y-%m-%d\")\n",
    "    return (ratio, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60783f45",
   "metadata": {},
   "source": [
    "5) Now run the code to prepare the `userRisk` data and call the `debiasAndScale` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "datesUserRisk = []\n",
    "ratios = []\n",
    "consolidatedDates = []\n",
    "flag = 0\n",
    "\n",
    "for i in range(len(convertedUserRisk['rawData'])):\n",
    "    date = datetime.strptime(convertedUserRisk['rawData'][i]['aggregation_start_time'][0:10], \"%Y-%m-%d\")\n",
    "    \n",
    "    # Group Dates and Prepare the Adjustment Ratios\n",
    "    if date in datesUserRisk and i != (len(convertedUserRisk['rawData']) - 1):\n",
    "        datesUserRisk.append(date)\n",
    "    elif (date not in datesUserRisk and i > 0) or (i == (len(convertedUserRisk['rawData']) - 1)):\n",
    "        ratio, returnedDate = debiasAndScale(convertedUserRisk, flag, i)\n",
    "        ratios.append(ratio)\n",
    "        consolidatedDates.append(returnedDate)\n",
    "        datesUserRisk.append(date)\n",
    "        flag = i\n",
    "    elif i == 0:\n",
    "        datesUserRisk.append(date)\n",
    "        \n",
    "UserRiskData = pd.DataFrame(list(zip(consolidatedDates, ratios)), columns = ['Date', 'UserRisk Ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d078e",
   "metadata": {},
   "source": [
    "Here is what the `userRisk` ratio data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UserRiskData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a59a7b9",
   "metadata": {},
   "source": [
    "6) All that is left is combining the datasets appropriately. Recall that the `userRisk` adjustment ratio should be applied only to the Apple data, not the Google data. Multiply the adjustment ratio by the Apple notification counts to receive the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54adfa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "MergedData = DebiasedData.merge(UserRiskData, on = 'Date', how = 'left')\n",
    "MergedData['Corrected Apple Notifications'] = MergedData['Apple Notifications'] * MergedData['UserRisk Ratio']\n",
    "MergedData['Total Notifications'] = MergedData['Corrected Apple Notifications'] + MergedData['Google Notifications']\n",
    "MergedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175a478",
   "metadata": {},
   "source": [
    "## Visualizing the Debiased Data\n",
    "\n",
    "This section will describe how to prepare and visualize the raw data downloaded from ENPA's REST API. Begin by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404c075",
   "metadata": {},
   "source": [
    "1) Run the code below to construct the bar chart. The dataframe, `MergedData`, is already organized, so it is easy to plot the total notifications over time. We can use a stacked bar to show the number of notifications from Apple/iOS devices or Google/Android devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270d79c",
   "metadata": {},
   "source": [
    "2) Next, save the plot to your current directory. The filename will include the date range of the plot (e.g., `TotalNotificationsOverTime_2022-01-01_to_2022-01-31.png`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plotting a Stacked Bar\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "barWidth = 0.6\n",
    "plt.bar(MergedData['Date'], MergedData['Corrected Apple Notifications'], barWidth, label = 'iOS')\n",
    "plt.bar(MergedData['Date'], MergedData['Google Notifications'], barWidth, label = 'Android', bottom = MergedData['Corrected Apple Notifications'])\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Notifications\")\n",
    "plt.title(\"Total Notifications Over Time\")\n",
    "\n",
    "startDate = datetime.strftime(MergedData['Date'][0], \"%Y-%m-%d\")\n",
    "endDate = datetime.strftime(MergedData['Date'][len(MergedData['Date']) - 1], \"%Y-%m-%d\")\n",
    "\n",
    "plt.savefig(f'TotalNotificationsOverTime_{startDate}_to_{endDate}.png', format='png', dpi=400)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c1346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
